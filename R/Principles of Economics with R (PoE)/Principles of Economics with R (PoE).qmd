---
title: "Principles of Economics with R (PoE)"
author: ryannthegeek
date: last-modified
format:
  pdf:
    toc: true
    toc-depth: 4
    toccolor: blue
    number-sections: true
    number-depth: 3
    lof: true
    lot: true
    urlcolor: purple
    listings: true
    include-in-header: listset.tex
    latex-auto-install: true
editor: visual
execute: 
  warning: false
  error: false
  cache: false
---

# The Simple Linear Regression Model

## The Simple Linear Regression Model

A simple linear regression model assumes that a linear relationship exists between the conditional expectation of a dependent variable $y$ and an independent variable $x$.

The assumed relationship in a linear regression model has the form:

$$y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i}$$ {#eq-linea-regression-model} where:

-   $y$ is the dependent variable
-   $x$ is the independent variable
-   $e$ is an error term
-   $\sigma^2$ is the variance of the error term
-   $\beta_0$ is the intercept parameter or coefficient
-   $\beta_1$ is the slope parameter or coefficient
-   $i$ stands for the $i^{th}$ observation in the data set, $i=1,2,...,N$
-   $N$ is the number of observations in the data set.

The *predicted*, or estimated value of $y$ given $x$ is given by:

$$\hat{y}=\beta_0+\beta_1x$$

### Assumptions of simple linear model

-   The values of $x$ are previously chosen (therefore, they are non-random).
-   The variance of the error term $\sigma^2$ is the same for all values of $x$.
-   There is no connection between one observation and another (no correlation between the error terms of two observations).
-   The expected value of the error term for any value of $x$ is zero.
-   The error term is normally distributed.

```{r}
#| fig-cap: A plot of wage against education
require(PoEdata)
data("cps_small")
attach(cps_small)
names(cps_small)
require(ggplot2)
ggplot() +
  geom_point(data = cps_small, aes(x = educ, y = wage)) +
  ggtitle("A plot of wage against education") +
  xlab("Education") +
  ylab("Wage")
```

## Example: Food Expenditure versus Income

```{r}
#| fig-cap: A scatter diagram for the food expenditure versus income
data("food")
attach(food)
names(food)
max(food_exp);max(income)

ggplot() +
  geom_point(data = food, aes(x = income, y = food_exp)) +
  scale_x_continuous(name = "weekly income in $100", limits = c(0, 34)) +
  scale_y_continuous(name = "weekly food expenditure in $", limits = c(0, 588)) +
  ggtitle("A scatter plot of food expenditure against income")
```

## Estimating a Linear Regression

$$\text{foodexpenditure}=\beta_0+\beta_1\text{income}+e$$

```{r}
m1 <- lm(food_exp ~ income, data = food)
b0 <- coef(m1)[[1]]
b1 <- coef(m1)[[2]]
summary_m1 <- summary(m1);summary_m1
coef(m1)
```

The intercept parameter $\beta_0$ is usually of little importance in econometric models; we are mostly interested in the slope parameter $\beta_1$.\
The estimated value of $\beta_1$ suggests that the **food expenditure** for an average family increases by $10.209643$ when the **family income** increases by $1$ unit, which in this case is \$100.\
The R function `geom_abline()` adds the regression line.

```{r}
#| fig-cap: A regression on food expenditure against income
ggplot() +
  geom_point(data = food, aes(x = income, y = food_exp)) +
  scale_x_continuous(name = "weekly income in $100", limits = c(0, 34)) +
  scale_y_continuous(name = "weekly food expenditure in $", limits = c(0, 588)) +
  geom_abline(intercept = b0, slope = b1, color = "skyblue", linetype = "solid", size = 1.5) +
  ggtitle("A regression on food expenditure against income")
```

list the names of all results in each object

```{r}
names(m1)
names(summary_m1)
m1$coefficients
summary_m1$coefficients
```

## Prediction with the Linear Regression Model

```{r}
newx <- data.frame(income = c(20, 25, 27))
yhat <- predict(m1, newx)
names(yhat) <- c("income=$2000", "$2500", "$2700") 
yhat  # prints the result
```

## Repeated Samples to Assess Regression Coefficients

Let us construct a number of random sub samples from the food data and re-calculate $\beta_0$ and $\beta_1$. A random sub sample can be constructed using the function sample(), as the following example illustrates only for $\beta_1$.

```{r}
N <- nrow(food);N # returns the number of observations in the dataset
C <- 50         # desired number of subsamples
S <- 38         # desired sample size

sumb1 <- 0 # initial value
for (i in 1:C){   # a loop over the number of subsamples
  set.seed(3*i)   # a different seed for each subsample  
  subsample <- food[sample(1:N, size = S, replace = TRUE), ]
  m2 <- lm(food_exp ~ income, data = subsample)
  #sum b2 for all subsamples:
  sumb1 <- sumb1 + coef(m2)[[2]]
}
print(sumb1/C, digits = 3)
```

The result, $\beta_1= 9.88$, is the average of $50$ estimates of $\beta_1$

## Estimated Variances and Covariance of Regression Coefficients

Many applications require estimates of the variances and covariances of the regression coefficients. R stores them in the a `matrix vcov()`:

```{r}
varb0 <- vcov(m1)[1, 1];varb0
varb1 <- vcov(m1)[2, 2];varb1
covb0b1 <- vcov(m1)[1,2];covb0b1
```

## Non-Linear Relationships

### The quadratic model

The quadratic model requires the square of the independent variable.

$$y_i=\beta_0+\beta_1x^2_i+e_i$$

In R, independent variables involving mathematical operators can be included in a regression equation with the function `I()`.\
The following example uses the dataset `br` from the package `PoEdata`, which includes the `sale prices` and the `surface area in square feet` of $1080$ houses in Baton Rouge, LA.\
`Price` is the `sale price in dollars`, and `sqft` is the `surface area in square feet`.

```{r}
#| fig-subcap:
#|   - A scatter plot of sale price of 1080 houses in Baton Rouge, LA against square feet
#|   - Fitting a quadratic model to the br dataset
#|   - Fitting a quadratic model to the br dataset by specifying se = F
data(br) # sometimes attach() function doesn't work, use data() # just always use both!!!
attach(br)

ggplot() +
  geom_point(data = br, aes(x = sqft, y = price)) +
  xlab("Totalsquare feet") +
  ylab("Sale price in $") +
  ggtitle("A scatter plot of sale price of 1080 houses in Baton Rouge, LA against square feet")

m3 <- lm(price ~ I(sqft^2), data = br)
b0 <- coef(m3)[[1]]
b1 <- coef(m3)[[2]]
sqftx = c(2000, 4000, 6000) # given values for sqft
pricex = b0 + b1*sqftx^2 # prices corresponding to given sqft 
DpriceDsqft <- 2*b1*sqftx # marginal effect of sqft on price
elasticity = DpriceDsqft*sqftx/pricex 
par.df <- data.frame(b0, b1);par.df
data.df <- data.frame(sqftx, pricex, DpriceDsqft, elasticity);data.df

## draw a scatter diagram and see how the quadratic function fits the data

ggplot(data = br, aes(x = sqft, y = price)) +
  geom_point() + # add the quadratic curve to the scatter plot
  geom_smooth(method = "lm", formula = y ~ x + I(x^2)) +
  xlab("Totalsquare feet") +
  ylab("Sale price in $") +
  ggtitle("Fitting a quadratic model to the br dataset")

## we can remove the confidence interval by specifying se = F

ggplot(data = br, aes(x = sqft, y = price)) +
  geom_point() + # add the quadratic curve to the scatter plot
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = F) +
  xlab("Totalsquare feet") +
  ylab("Sale price in $") +
  ggtitle("Fitting a quadratic model to the br dataset")
```

### The log-linear model

The log-linear model regresses the log of the dependent variable on a linear expression of the independent variable.\
The log linear model is given by:

$$log(y_i)=\beta_0+\beta_1x_i+e_i$$

One of the reasons to use the log of an independent variable is to make its distribution closer to the normal distribution

#### Histogram of price

```{r}
#| fig-cap: Histogram of price
ggplot(data = br, aes(x = price)) +
  geom_histogram()
```

#### Histogram of log price

```{r}
#| fig-cap: Histogram of price
ggplot(data = br, aes(x = log(price))) +
  geom_histogram()
##  It can be noticed that that the log is closer to the normal distribution.
```

We are interested in the *estimates* of the *coefficients* and their interpretation, in the *fitted values* of price, and in the *marginal effect* of an increase in sqft on price.

```{r}
m4 <- lm(log(price) ~ sqft, data = br)
coef(m4)
summary(m4)
```

The coefficients are $\beta_0 = 10.84$ and $\beta_1 = 0.00041$ , showing that an increase in the surface area (sqft) of an apartment by one unit (1 sqft) increases the price of the apartment by 0.041%. Thus, for a house price of \$100,000, an increase of 100 sqft will increase the price by approximately 100(0.041)%, which is equal to $4112.7$.\
In general, the marginal effect of an increase in $x$ on $y$ is

$$\frac{dy}{dx}=\beta_1y$$ and the elasticity is:

$$\epsilon=\frac{dy}{dx}\frac{x}{y}=\beta_1x$$

#### Drawing the fitted values curve of the log-linear model

```{r}
#| fig-cap: Fitting a quadratic model to the br dataset
ggplot(data = br, aes(x = sqft, y = price)) +
  geom_point() + # add the quadratic curve to the scatter plot
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = F) +
  xlab("Totalsquare feet") +
  ylab("Sale price in $") +
  ggtitle("Fitting a quadratic model to the br dataset")

ordat <- br[order(br$sqft), ] #order the dataset
plot(br$sqft, br$price, col = "grey")
lines(exp(fitted(m4)) ~ ordat$sqft,
      col = "blue", main = "Log-linear Model")

b0 <- coef(m4)[[1]]
b1 <- coef(m4)[[2]]
#pick a few values for sqft:
sqftx <- c(2000, 3000, 4000) 
#estimate prices for those and add one more:
pricex <- c(100000, exp(b0+b1*sqftx)) 
#re-calculate sqft for all prices:
sqftx <- (log(pricex)-b0)/b1
```

**calculate and print elasticities:**

```{r}
(elasticities <- b1*sqftx) # the brackets makes sure it is printed
```

## Using Indicator Variables in a Regression

An indicator, or binary variable marks the presence or the absence of some attribute of the observational unit, such as gender or race if the observational unit is an individual, or location if the observational unit is a house. In the data set utown, the variable utown is $1$ if a house is close to the university and $0$ otherwise. Here is a simple linear regression model that involves the variable `utown`:

$$\text{price}_i=\beta_0+\beta_1\text{utown}_i$$ {#eq-price-utown}

The coefficient of such a variable in a simple linear model is equal to the difference between the average prices of the two categories; the intercept coefficient of the model is equal to the average price of the houses that are not close to university.

```{r}
data("utown")
attach(utown)
price0bar <- mean(utown$price[which(utown$utown == 0)])
price1bar <- mean(utown$price[which(utown$utown == 1)])
prices.df <- data.frame(price0bar, price1bar);prices.df
```

The results are: $\overline{\text { price }}=277.24$ close to university, and $\overline{\text { price }}=215.73$ for those not close.

### fitting a regression model

I now show that the same results yield the coefficients of the regression model

$$\text{price}_i=\beta_0+\beta_1\text{utown}_i$$

```{r}
m5 <- lm(price ~ utown, data = utown)
b0 <- coef(m5)[[1]] 
b1 <- coef(m5)[[2]]
```

The results are: $\overline{\text { price }}=\beta_1=215.73$ for non-university houses, and $\overline{\text { price }}=\beta_0+\beta_1=277.24$ for university houses.

## Monte Carlo Simulation
